@webpage{annoucment,
   title = {Introducing Hand Tracking on Oculus Quest—Bringing Your Real Hands into VR},
   url = {https://www.oculus.com/blog/introducing-hand-tracking-on-oculus-quest-bringing-your-real-hands-into-vr/},
}
@article{Englmeier,
   abstract = {(a) (c) (b) (d) Figure 1: We compare two VR teleportation techniques: a planar (a) and a spherical (b) World in Miniature. The planar WIM relies on button-supported interaction (c) while the SWIM is solely controlled and embodied with a physical sphere (d). We evaluate both techniques with the tasks of scrolling, scaling, and teleportation, each with two different display sizes. ABSTRACT We explore the concept of a Spherical World in Miniature (SWIM) for discrete locomotion in Virtual Reality (VR). A SWIM wraps a planar WIM around a physically embodied sphere and thereby implements the metaphor of a tangible Tiny Planet that can be rotated and moved, enabling scrolling, scaling, and avatar teleportation. The scaling factor is set according to the sphere's distance from the head-mounted display (HMD), while rotation moves the current viewing window. Teleportation is triggered with a dwell time when looking at the sphere and keeping it still. In a lab study (N=20), we compare our SWIM implementation to a planar WIM with an established VR controller technique using physical buttons. We test both concepts in a navigation task and also investigate the effects of two different screen sizes. Our results show that the SWIM, despite its less direct geometrical transformation, performed superior in most evaluations. It outperformed the planar WIM not only in terms of task completion time (TCT) and accuracy but also in subjective ratings.},
   author = {David Englmeier and Wanja Sajko and Andreas Butz},
   keywords = {Index Terms},
   title = {Spherical World in Miniature: Exploring the Tiny Planets Metaphor for Discrete Locomotion in Virtual Reality},
}
@webpage{walkingstick,
   title = {Walking Stick VR Locomotion (download in description!) : Vive},
   url = {https://www.reddit.com/r/Vive/comments/gwiz3w/walking_stick_vr_locomotion_download_in/},
}
@inproceedings{Pei2022,
   author = {Siyou Pei and Alexander Chen and Jaewook Lee and Yang Zhang},
   city = {New York, NY, USA},
   doi = {10.1145/3491102.3501898},
   isbn = {9781450391573},
   journal = {CHI Conference on Human Factors in Computing Systems},
   month = {4},
   pages = {1-16},
   publisher = {ACM},
   title = {Hand Interfaces: Using Hands to Imitate Objects in AR/VR for Expressive Interactions},
   url = {https://dl.acm.org/doi/10.1145/3491102.3501898},
   year = {2022},
}
@article{Andrii2022,
   abstract = {Figure 1: Two possible scenarios for teleportation in 3D space: a user is teleporting horizontally to a target using a parabolic aiming method (left) and a user is teleporting vertically to a target using a linear aiming method (right). ABSTRACT Teleportation has become the de facto standard of locomotion in Virtual Reality (VR) environments. However, teleportation with parabolic and linear target aiming methods is restricted to horizontal 2D planes and it is unknown how they transfer to the 3D space. In this paper, we propose six 3D teleportation methods in virtual environments based on the combination of two existing aiming methods (linear and parabolic) and three types of transitioning to a target (instant, interpolated and continuous). To investigate the performance of the proposed teleportation methods, we conducted a controlled lab experiment (N = 24) with a mid-air coin collection task to assess accuracy, efciency and VR sickness. We discovered that the linear aiming method leads to faster and more accurate target selection. Moreover, a combination of linear aiming and instant transitioning leads to the highest efciency and accuracy without increasing VR sickness. • Human-centered computing → Virtual reality; User studies ; Empirical studies in HCI.},
   author = {Andrii Matviienko Florian Müller Martin Schmitz and Marco Fendrich Max Mühlhäuser and Andrii Matviienko and Florian Müller and Martin Schmitz and Marco Fendrich},
   city = {New York, NY, USA},
   doi = {10.1145/3491102.3501983},
   isbn = {9781450391573},
   journal = {CHI Conference on Human Factors in Computing Systems},
   keywords = {locomotion,teleportation,virtual environments,virtual reality},
   month = {4},
   pages = {1-11},
   publisher = {ACM},
   title = {SkyPort: Investigating 3D Teleportation Methods in Virtual Environments},
   url = {https://dl.acm.org/doi/10.1145/3491102.3501983},
   year = {2022},
}
@article{Laugwitz2008,
   abstract = {An end-user questionnaire to measure user experience quickly in a simple and immediate way while covering a preferably comprehensive impression of the product user experience was the goal of the reported construction process. An empirical approach for the item selection was used to ensure practical relevance of items. Usability experts collected terms and statements on user experience and usability, including 'hard' as well as 'soft' aspects. These statements were consolidated and transformed into a first questionnaire version containing 80 bipolar items. It was used to measure the user experience of software products in several empirical studies. Data were subjected to a factor analysis which resulted in the construction of a 26 item questionnaire including the six factors Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and Novelty. Studies conducted for the original German questionnaire and an English version indicate a satisfactory level of reliability and construct validity. © 2008 Springer Berlin Heidelberg.},
   author = {Bettina Laugwitz and Theo Held and Martin Schrepp},
   doi = {10.1007/978-3-540-89350-9_6},
   isbn = {3540893490},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Perceived usability,Questionnaire,Software evaluation,Usability assessment,User experience,User satisfaction},
   pages = {63-76},
   publisher = {Springer Verlag},
   title = {Construction and evaluation of a user experience questionnaire},
   volume = {5298 LNCS},
   year = {2008},
}
@article{Santiago2020,
   abstract = {Gesture elicitation studies represent a popular and resourceful method in HCI to inform the design of intuitive gesture commands , reflective of end-users' behavior, for controlling all kinds of interactive devices, applications, and systems. In the last ten years, an impressive body of work has been published on this topic, disseminating useful design knowledge regarding users' preferences for finger, hand, wrist, arm, head, leg, foot, and whole-body gestures. In this paper, we deliver a systematic literature review of this large body of work by summarizing the characteristics and findings of N=216 gesture elicitation studies subsuming 5,458 participants, 3,625 referents, and 148,340 elicited gestures. We highlight the descriptive, comparative , and generative virtues of our examination to provide practitioners with an effective method to (i) understand how new gesture elicitation studies position in the literature; (ii) compare studies from different authors; and (iii) identify opportunities for new research. We make our large corpus of papers accessible online as a Zotero group library at https://www. zotero.org/groups/2132650/gesture_elicitation_studies.},
   author = {Santiago Villarreal-Narvaez and Jean Vanderdonckt and Radu-Daniel Vatavu and Jacob O Wobbrock},
   doi = {10.1145/3357236.3395511},
   isbn = {9781450369749},
   keywords = {Author Keywords Gesture elicitation,Survey,Systematic Literature Review CCS Concepts},
   title = {A Systematic Review of Gesture Elicitation Studies: What Can We Learn from 216 Studies?},
   url = {http://dx.doi.org/10.1145/3357236.3395511},
   year = {2020},
}
@article{Mayer2020,
   abstract = {Collaborative Virtual Environments (CVEs) offer unique opportunities for human communication. Humans can interact with each other over a distance in any environment and visual embodiment they want. Although deictic gestures are especially important as they can guide other humans' attention, humans make systematic errors when using and interpreting them. Recent work suggests that the interpretation of vertical deictic gestures can be significantly improved by warping the pointing arm. In this paper, we extend previous work by showing that models enable to also improve the interpretation of deictic gestures at targets all around the user. Through a study with 28 participants in a CVE, we analyzed the errors users make when interpreting deictic gestures. We derived a model that rotates the arm of a pointing user's avatar to improve the observing users' accuracy. A second study with 24 participants shows that we can improve observers' accuracy by 22.9\%. As our approach is not noticeable for users, it improves their accuracy without requiring them to learn a new interaction technique or distracting from the experience.},
   author = {Sven Mayer and Jens Reinhardt and Robin Schweigert and Brighten Jelke and Valentin Schwind and Katrin Wolf and Niels Henze},
   doi = {10.1145/3313831.3376340},
   isbn = {9781450367080},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {correction model,deictic,ray tracing,virtual reality},
   month = {4},
   publisher = {Association for Computing Machinery},
   title = {Improving Humans' Ability to Interpret Deictic Gestures in Virtual Reality},
   url = {http://dx.doi.org/10.1145/3313831.3376340},
   year = {2020},
}
@article{Rebelo2020,
   abstract = {In human behavior studies in critical situations, such as emergency situations, it’s necessary for the participant to feel that is in danger and that behaves the same way as in the real world. For this behavior to be natural, the technique to control the avatar...},
   author = {Inês Galrão and Francisco Rebelo and Paulo Noriega},
   doi = {10.1007/978-3-030-20227-9_40},
   isbn = {9783030202262},
   issn = {21945365},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Avatar control,Cognitive maps,Locomotion-in-place,Teleport,Virtual environments,Virtual reality},
   pages = {436-445},
   publisher = {Springer, Cham},
   title = {Locomotion-in-Place and Teleport: Which Is the Best Technique to Be Used in Human Behavior Research Using Virtual Reality?},
   volume = {955},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-20227-9_40},
   year = {2020},
}
@webpage{bio,
   title = {ANTHROPOMETRY AND BIOMECHANICS},
   url = {https://msis.jsc.nasa.gov/sections/section03.htm},
}
@webpage{comfort,
   title = {(PDF) Hand discomfort in production assembly workers},
   url = {https://www.researchgate.net/publication/237844929_Hand_discomfort_in_production_assembly_workers/figures?lo=1},
}
@article{Karre2020,
   abstract = {VR development practices have a diverse set of practices compared to traditional software development. Tasks like scene design, acoustic design, vergence manipulation, image depth, etc. are specific to VR apps and hence require evaluation processes that may be different from the traditional means. Usability Evaluation is one such process which is being executed in an unconventional way by Industrial Practitioners today. In this paper, the researchers detail a Systematic Literature Review of the Usability Evaluation Methods practised by Industrial researchers while building VR Products. The researchers found that VR Product teams follow unique methods to improve usability in their products. Further, the researchers consolidate these methods and provide insights into choosing the best to build a real-world VR Product based on the defined product constraints},
   author = {Sai Anirudh Karre and Neeraj Mathur and Y. Raghu Reddy},
   doi = {10.1145/3381307.3381309},
   issn = {1559-6915},
   issue = {4},
   journal = {ACM SIGAPP Applied Computing Review},
   month = {1},
   pages = {17-27},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Understanding usability evaluation setup for VR products in industry},
   volume = {19},
   year = {2020},
}
@article{Özgen2019,
   abstract = {Virtual reality (VR) is an emerging technology that is being used in a wide range of fields such as medicine, gaming, psychology and sociology. The use of VR is promising in the field of education and requires investigation, but research on the use of VR in education is still limited. This enables the exploration of new territories, and design education is one of them. Design education, an important part of the curriculum of architecture students who aim to conceptualize problem-solving, is still taught using traditional methodologies with touches of digital technologies. Thus, there is limited research into the implementation of VR. This study proposes using VR in basic design education and focuses on the usability of VR, especially for problem-solving activities. It presents the literature on basic design education of digital approaches, VR technologies, usability criteria and the technology acceptance model. In order to analyse the usability of VR, we conducted an experimental study with 20 first-year interior architecture and architecture students. We found that, statistically, there is a significant difference in terms of ‘the intention to use’ and ‘the perceived enjoyment’ between the VR group and the paper-based group. Moreover, there is, statistically, a difference in effectiveness within the paper-based group and the VR-based group when one compares the success of two types of design problems in the same group. Thus, one can summarize that using VR can strongly enhance problem-solving activities in interior architecture and for architecture students and that one can consider it to be a promising and complementary tool in basic design education.},
   author = {Dilay Seda Özgen and Yasemin Afacan and Elif Sürer},
   doi = {10.1007/S10798-019-09554-0},
   issn = {1573-1804},
   issue = {2},
   journal = {International Journal of Technology and Design Education 2019 31:2},
   keywords = {Educational Technology,Learning and Instruction,Science Education,Virtual reality},
   month = {11},
   pages = {357-377},
   publisher = {Springer},
   title = {Usability of virtual reality for basic design education: a comparative study with paper-based design},
   volume = {31},
   url = {https://link.springer.com/article/10.1007/s10798-019-09554-0},
   year = {2019},
}
@webpage{Magnopus,
   author = {Magnopus},
   title = {Elixir für Oculus Quest 2 | Oculus},
   url = {https://www.oculus.com/experiences/quest/3793077684043441/?locale=de_DE},
}
@webpage{Ultraleap,
   title = {World-leading Hand Tracking: Small. Fast. Accurate. | Ultraleap},
   url = {https://www.ultraleap.com/tracking/},
}
@webpage{github,
   author = {jorgejgnz},
   title = {jorgejgnz/HandTrackingGestureRecorder: Unity script to record any gesture with your own hands},
   url = {https://github.com/jorgejgnz/HandTrackingGestureRecorder#readme},
   year = {2021},
}
@article{Vatavu2015,
   abstract = {We address in this work the process of agreement rate analysis for characterizing the level of consensus between participants' proposals elicited during guessability studies. Two new measures, i.e., disagreement rate for referents and coagreement rate between referents, are proposed to accompany the widelyused agreement rate formula of Wobbrock et al. [37] when reporting participants' consensus for symbolic input. A statistical significance test for comparing the agreement rates of k2 referents is presented in analogy with Cochran's success/ failure Q test [5], for which we express the test statistic in terms of agreement and coagreement rates. We deliver a toolkit to assist practitioners to compute agreement, disagreement, and coagreement rates, and run statistical tests for agreement rates at p=:05, :01, and :001 levels of significance. We validate our theoretical development of agreement rate analysis in relation with several previously published elicitation studies. For example, when we present the probability distribution function of the agreement rate measure, we also use it (1) to explain the magnitude of agreement rates previously reported in the literature, and (2) to propose qualitative interpretations for agreement rates, in analogy with Cohen's guidelines for effect sizes [6]. We also re-examine previously published elicitation data from the perspective of the agreement rate test statistic, and highlight new findings on the effect of referents over agreement rates, unattainable prior to this work. We hope that our contributions will advance the current knowledge in agreement rate analysis, providing researchers and practitioners with new techniques and tools to help them understand user-elicited data at deeper levels of detail and sophistication.},
   author = {Radu Daniel Vatavu and Jacob O. Wobbrock},
   doi = {10.1145/2702123.2702223},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Agreement rate,Coagreement,Disagreement,Guessability study,Methodology,Statistical test,User-defined gestures},
   month = {4},
   pages = {1325-1334},
   publisher = {Association for Computing Machinery},
   title = {Formalizing agreement analysis for elicitation studies: New measures, significance test, and toolkit},
   volume = {2015-April},
   year = {2015},
}
@article{Vogiatzidakis2018,
   abstract = {Mid-air interaction involves touchless manipulations of digital content or remote devices, based on sensor tracking of body movements and gestures. There are no established, universal gesture vocabularies for mid-air interactions with digital content or remote devices based on sensor tracking of body movements and gestures. On the contrary, it is widely acknowledged that the identification of appropriate gestures depends on the context of use, thus the identification of mid-air gestures is an important design decision. The method of gesture elicitation is increasingly applied by designers to help them identify appropriate gesture sets for mid-air applications. This paper presents a review of elicitation studies in mid-air interaction based on a selected set of 47 papers published within 2011\&ndash;2018. It reports on: (1) the application domains of mid-air interactions examined; (2) the level of technological maturity of systems at hand; (3) the gesture elicitation procedure and its variations; (4) the appropriateness criteria for a gesture; (5) participants number and profile; (6) user evaluation methods (of the gesture vocabulary); (7) data analysis and related metrics. This paper confirms that the elicitation method has been applied extensively but with variability and some ambiguity and discusses under-explored research questions and potential improvements of related research.},
   author = {Panagiotis Vogiatzidakis and Panayiotis Koutsabasis},
   doi = {10.3390/MTI2040065},
   issue = {4},
   journal = {Multimodal Technologies and Interaction 2018, Vol. 2, Page 65},
   keywords = {air interaction,elicitation method,gesture,mid,review,survey},
   month = {9},
   pages = {65},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Gesture Elicitation Studies for Mid-Air Interaction: A Review},
   volume = {2},
   url = {https://www.mdpi.com/2414-4088/2/4/65/htm https://www.mdpi.com/2414-4088/2/4/65},
   year = {2018},
}
@article{elicitation,
   abstract = {Gesture elicitation studies represent a popular and resourceful method in HCI to inform the design of intuitive gesture commands, reflective of end-users' behavior, for controlling all kinds of interactive devices, applications, and systems. In the last ten years, an impressive body of work has been published on this topic, disseminating useful design knowledge regarding users' preferences for finger, hand, wrist, arm, head, leg, foot, and whole-body gestures. In this paper, we deliver a systematic literature review of this large body of work by summarizing the characteristics and findings ofN=216gesture elicitation studies subsuming 5,458 participants, 3,625 referents, and 148,340 elicited gestures. We highlight the descriptive, comparative, and generative virtues of our examination to provide practitioners with an effective method to (i) understand how new gesture elicitation studies position in the literature; (ii) compare studies from different authors; and (iii) identify opportunities for new research.},
   author = {Santiago Villarreal-Narvaez and Jean Vanderdonckt and Radu Daniel Vatavu and Jacob O. Wobbrock},
   doi = {10.1145/3357236.3395511},
   journal = {DIS 2020 - Proceedings of the 2020 ACM Designing Interactive Systems Conference},
   keywords = {Gesture elicitation,Survey,Systematic literature review},
   month = {7},
   pages = {855-872},
   publisher = {Association for Computing Machinery, Inc},
   title = {A systematic review of gesture elicitation studies: What can we learn from 216 studies?},
   year = {2020},
}
@inproceedings{Tanaka2020,
   abstract = {Motion sickness is one of the most common issues that affects the user experience in immersive virtual reality environments. In general, a user feels motion sickness because the brain perceives a movement but his/her body is not actually moving. In immersive virtual reality, this is caused because, frequently, the user's field of view is blocked by a head-mounted display and the locomotion in the virtual world is performed by moving analog sticks in a gamepad-like device (this is also known as artificial locomotion). In addition to some general best practices and guidelines to prevent motion sickness and provide immersion, several different locomotion mechanisms have been conceived in the last few years, both in the academy and in the industry. This paper presents the findings from usability tests to compare different locomotion mechanisms. The evaluation took into account not only the level of motion sickness observed in the participants, but also the ease of learn and use, the subjective preferences and the sense of immersion. It is expected that the data from these tests will assist the development of immersive virtual reality environments and future enhancements of locomotion mechanisms for immersive virtual reality.},
   author = {Eduardo Tanaka and Tiago Paula and Alisson Silva and Victor Antunes and Leonardo Domingues and Lucimara De Almeida and Alex Alves and Roberta Oliveira},
   doi = {10.1145/3424953.3426550},
   isbn = {9781450381727},
   journal = {IHC 2020 - Proceedings of the 19th Brazilian Symposium on Human Factors in Computing Systems},
   keywords = {locomotion,usability testing,virtual reality},
   month = {10},
   publisher = {Association for Computing Machinery, Inc},
   title = {Evaluation of immersive virtual reality locomotion mechanisms},
   year = {2020},
}
@inproceedings{Osawa2008,
   abstract = {Two-handed control techniques for precisely and efficiently manipulating a virtual 3D object by hand in an immersive virtual reality environment are proposed. In addition, one-handed and two-handed techniques are described and comparatively evaluated. The techniques are used to precisely control and efficiently adjust an object with the speed of one hand or the distance between both hands. The controlled adjustments are actually position and viewpoint adjustments. The results from experimental evaluations show that two-handed control methods that are used to make the position and viewpoint adjustments are the best, but the simultaneous use of both one-handed and two-handed control techniques does not necessarily improve the usability. © Springer-Verlag Berlin Heidelberg 2008.},
   author = {Noritaka Osawa},
   doi = {10.1007/978-3-540-89639-5_94},
   isbn = {3540896384},
   issn = {03029743},
   issue = {PART 1},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {987-997},
   title = {Two-handed and one-handed techniques for precise and efficient manipulation in immersive virtual environments},
   volume = {5358 LNCS},
   year = {2008},
}
@article{Sagayam2017,
   abstract = {Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined.},
   author = {K. Martin Sagayam and D. Jude Hemanth},
   doi = {10.1007/s10055-016-0301-0},
   issn = {14349957},
   issue = {2},
   journal = {Virtual Reality},
   keywords = {Gesture,Graphical user interface (GUI),HMM,Human computer interaction (HCI),Posture},
   month = {6},
   pages = {91-107},
   publisher = {Springer London},
   title = {Hand posture and gesture recognition techniques for virtual reality applications: a survey},
   volume = {21},
   year = {2017},
}
@inproceedings{Paulmann2021,
   abstract = {We introduce a seamless locomotion concept designed for a commercial VR game. The locomotion concept was designed with the following goal: The whole game can be explored entirely without teleportation or joystick steering but only by natural locomotion. This should create a seamless fusion of exploration and discovery and, therefore, maximize presence and accessibility. Therefore, our game leverages a mix of different techniques: 1) Redirected Walking techniques such as rotation gains and impossible spaces, 2) Passive travel like lifts, moving platforms, vehicles, etc., 3) Indirect movement such as climbing, pulling a rope to move a raft, etc. Furthermore, these techniques are extended by or coupled with physical interactions or movements like crawling, jumping or sliding around corners, to distract player during the manipulation. The seamless integration of all these techniques enables the player to walk around freely in the recommended room-scale play space of 2x2 meters, while exploring a potentially infinite virtual space.},
   author = {Hannah Paulmann and Tim Mayer and Marc Barnes and Dennis Briddigkeit and Frank Steinicke and Eike Langbehn},
   doi = {10.1109/VRW52623.2021.00079},
   journal = {Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021},
   title = {Combining natural techniques to achieve seamless locomotion in consumer VR spaces},
   year = {2021},
}
@article{Zhang2017,
   abstract = {In this paper, we present a double hand-gesture interaction (DHGI) method for walk-through in VR environment with an Oculus Rift headset and Leap Motion function. The user can control the avatar (first-person view) to move (walk/run) forward or backward by turning the user's left palm upward or downward, and by turning the avatar to the left or right with the right thumb pointing toward either direction. Compared with the results of the joystick input device and portal method using Oculus Rift Touches, the objective and subjective findings of this study indicate that DHGI is intuitive, easy to learn, easy to use, and causes low fatigue. Moreover, the user feedback shows that DHGI significantly improves immersion and reduces the sense of motion sickness in VR.},
   author = {Fan Zhang and Shaowei Chu and Ruifang Pan and Naye Ji and Lian Xi},
   doi = {10.1109/ICIS.2017.7960051},
   journal = {Proceedings - 16th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2017},
   keywords = {Double Hand-gesture Interaction (DHGI),Leap Motion,Virtual Reality,Walk-through},
   month = {6},
   pages = {539-544},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Double hand-gesture interaction for walk-through in VR environment},
   year = {2017},
}
@article{Bozgeyikli2016a,
   abstract = {Virtual reality (VR) has been used as an effective tool for training individuals with autism spectrum disorder (ASD). Recently there have been an increase in the number of applications developed for this purpose. One of the most important aspects of these applications is locomotion, which is an essential form of human computer interaction. Locomotion in VR has a direct effect on many aspects of user experience such as enjoyment, frustration, tiredness, motion sickness and presence. There have been many locomotion techniques proposed for VR. Most of them were designed and evaluated for neurotypical users. On the other hand, for individuals with ASD there isn't any study to our knowledge that focuses on locomotion techniques and their evaluation. In this study, eight locomotion techniques were implemented in an immersive virtual reality test environment. These eight VR locomotion techniques may be categorized as follows: three commonly used locomotion techniques (redirected walking, walk-in-place and joystick controller), two unexplored locomotion techniques (stepper machine and point \& teleport) and three locomotion techniques that were selected and designed for individuals with ASD based on their common characteristics (flying, flapping and trackball controller). A user study was performed with 12 high functioning individuals with ASD. Results indicated that joystick and point \& teleport techniques provided the most comfortable use for individuals with ASD, followed by walk in place and trackball. On the other hand, flying and hand flapping did not provide comfortable use for individuals with ASD.},
   author = {Evren Bozgeyikli and Andrew Raij and Srinivas Katkoori and Rajiv Dubey},
   city = {New York, NY, USA},
   doi = {10.1145/2983310},
   journal = {Proceedings of the 2016 Symposium on Spatial User Interaction},
   keywords = {Autism,Human Computer Interaction,Locomotion,Virtual Reality},
   publisher = {ACM},
   title = {Locomotion in Virtual Reality for Individuals with Autism Spectrum Disorder},
   url = {http://dx.doi.org/10.1145/2983310.2985763},
   year = {2016},
}
@article{bozgeyikli,
   abstract = {With the increasing popularity of virtual reality (VR) and new devices getting available with relatively lower costs, more and more video games have been developed recently. Most of these games use first person interaction techniques since it is more natural for Head Mounted Displays (HMDs). One of the most widely used interaction technique in VR video games is locomotion that is used to move user's viewpoint in virtual environments. Locomotion is an important component of video games since it can have a strong influence on user experience. In this study, a new locomotion technique we called "Point \& Teleport" is described and compared with two commonly used VR locomotion techniques of walk-in-place and joystick. In this technique, users simply point where they want to be in virtual world and they are teleported to that position. As a major advantage, it is not expected to introduce motion sickness since it does not involve any visible translational motion. In this study, two VR experiments were designed and performed to analyze the Point \& Teleport technique. In the first experiment, Point \& Teleport was compared with walkin-place and joystick locomotion techniques. In the second experiment, a direction component was added to the Point \& Teleport technique so that the users could specify their desired orientation as well. 16 users took part in both experiments. Results indicated that Point \& Teleport is a fun and user friendly locomotion method whereas the additional direction component degraded the user experience.},
   author = {Evren Bozgeyikli and Andrew Raij and Srinivas Katkoori and Rajiv Dubey},
   doi = {10.1145/2967934.2968105},
   journal = {CHI PLAY 2016 - Proceedings of the 2016 Annual Symposium on Computer-Human Interaction in Play},
   keywords = {Locomotion,Teleportation,Virtual reality},
   month = {10},
   pages = {205-216},
   publisher = {Association for Computing Machinery, Inc},
   title = {Point \& Teleport locomotion technique for virtual reality},
   year = {2016},
}
@article{Caggianese,
   abstract = {Virtual reality has achieved significant popularity in recent years, and allowing users to move freely within an immersive virtual world has become an important factor critical to realize. The user...},
   author = {Giuseppe Caggianese and Nicola Capece and Ugo Erra and Luigi Gallo and Michele Rinaldi},
   doi = {10.1080/10447318.2020.1785151},
   journal = {https://doi.org/10.1080/10447318.2020.1785151},
   pages = {1734-1755},
   publisher = {Taylor \& Francis},
   title = {Freehand-Steering Locomotion Techniques for Immersive Virtual Environments: A Comparative Evaluation},
   url = {https://www.tandfonline.com/doi/abs/10.1080/10447318.2020.1785151},
   year = {2020},
}
@article{Tcha-Tokey,
   abstract = {Most of the models of User eXperience (UX) in Immersive Virtual Environment (IVE) are partial due to the components and the measuring methods they suggest. We have presented in a previous work a holistic UX in IVE model, combining key components and influencing factors from the most common fields of Virtual Reality (education, entertainment and edutainment). We do not doubt that the best way to measure the UX in IVEs is to gather and compare results from the appropriate subjective methods with the appropriate objective methods. Nevertheless, in this paper, we chose to focus only on the questionnaire method. Indeed, most of components can be measured through questionnaires. The objective of this paper is to rely on the components of our UX model to select appropriate existing questionnaires and finally choose the more suitable items to create our own mixed questionnaire. First, this paper reviews the questionnaires chosen to create our own. Finally, it proposes an elaboration of the final questionnaire. CCS Concepts • Software and its engineering➝Virtual worlds training simulations • Computing methodologies➝Virtual reality.},
   author = {Katy Tcha-Tokey and Emilie Loup-Escande and Olivier Christmann and Simon Richir},
   city = {New York, NY, USA},
   doi = {10.1145/2927929},
   journal = {Proceedings of the 2016 Virtual Reality International Conference},
   keywords = {Model,Non-Instrumental Measures,Questionnaire,Subjective Methods,User Experience,Virtual Environment},
   publisher = {ACM},
   title = {A Questionnaire to Measure the User Experience in Immersive Virtual Environments},
   url = {http://dx.doi.org/10.1145/2927929.2927955},
}
@article{Jacob2008,
   abstract = {We are in the midst of an explosion of emerging human-computer interaction techniques that redefine our understanding of both computers and interaction. We propose the notion of Reality-Based Interaction (RBI) as a unifying concept that ties together a large subset of these emerging interaction styles. Based on this concept of RBI, we provide a framework that can be used to understand, compare, and relate current paths of recent HCI research as well as to analyze specific interaction designs. We believe that viewing interaction through the lens of RBI provides insights for design and uncovers gaps or opportunities for future research.},
   author = {Robert J K Jacob and Audrey Girouard and Leanne M Hirshfield and Michael S Horn and Orit Shaer and Erin Treacy Solovey and Jamie Zigelbaum},
   city = {New York, New York, USA},
   doi = {10.1145/1357054},
   journal = {Proceeding of the twenty-sixth annual CHI conference on Human factors in computing systems  - CHI '08},
   keywords = {Author Keywords Reality-Based Interaction,context-aware,interaction styles,multimodal,next-generation,post-WIMP interfaces ACM Classification Keywords H,tangible interfaces,ubiquitous computing,virtual reality},
   publisher = {ACM Press},
   title = {Reality-Based Interaction: A Framework for Post-WIMP Interfaces},
   year = {2008},
}
@webpage{WaWizTelePath,
   title = {Introducing Next Generation Telepath VR Movement Features | by Aldin | Aldin Blog | Medium},
   url = {https://medium.com/aldin-dynamics/introducing-next-generation-telepath-vr-movement-features-6417a7e0ff49},
}
@webpage{Aladin,
   title = {Aldin Blog – Medium},
   url = {https://medium.com/aldin-dynamics},
}
@webpage{WaWizBlog,
   title = {Natural Magic is Now live on the Oculus Store and Steam | by Aldin | Aldin Blog | Jul, 2021 | Medium},
   url = {https://medium.com/aldin-dynamics/natural-magic-is-now-live-on-the-oculus-store-and-steam-7a8115f35e1e},
}
@webpage{WaWizOculus,
   title = {Waltz of the Wizard: Natural Magic für Oculus Quest | Oculus},
   url = {https://www.oculus.com/experiences/quest/2280285932034855/},
}
@webpage{VacSimBlog,
   title = {How to Hands: A Developer Deep Dive on Hand Tracking in Vacation Simulator | Owlchemy Labs},
   url = {https://owlchemylabs.com/how-to-hands-a-developer-deep-dive-on-hand-tracking-in-vacation-simulator/},
}
@webpage{VacSimOculus,
   title = {Vacation Simulator für Oculus Quest | Oculus},
   url = {https://www.oculus.com/experiences/quest/2393300320759737/},
}
@article{Abtahi,
   abstract = {Figure 1: Left) Average human-size avatar walking in a large virtual city. Middle) Ground-Level Scaling technique used to achieve a 10x speed gain. Right) Eye-Level Scaling used to achieve a 10x speed gain, while maintaining a street-level view. ABSTRACT Advances in tracking technology and wireless headsets enable walking as a means of locomotion in Virtual Reality. When exploring virtual environments larger than room-scale, it is often desirable to increase users' perceived walking speed, for which we investigate three methods. (1) Ground-Level Scaling increases users' avatar size, allowing them to walk farther. (2) Eye-Level Scaling enables users to walk through a World in Miniature, while maintaining a street-level view. (3) Seven-League Boots amplifies users' movements along their walking path. We conduct a study comparing these methods and find that users feel most embodied using Ground-Level Scaling and consequently increase their stride length. Using Seven-League Boots, unlike the other two methods, diminishes positional accuracy at high gains, and users modify their walking behavior to compensate for the lack of control. We conclude with a discussion on each technique's strength and weaknesses and the types of situation they might be appropriate for.},
   author = {Parastoo Abtahi and Mar Gonzalez-Franco and Eyal Ofek and Anthony Steed},
   doi = {10.1145/3290605.3300752},
   isbn = {9781450359702},
   keywords = {CCS CONCEPTS • Human-centered computing → User stu,KEYWORDS Virtual Reality, Locomotion, Walking Spee,Virtual reality},
   title = {I'm a Giant: Walking in Large Virtual Environments at High Speed Gains},
   url = {https://doi.org/10.1145/3290605.3300752},
}
@report{Lin,
   abstract = {Figure 1: In our experiment, participants complete block stacking puzzles in virtual reality (left), controlling their avatar's hands either with tracked gloves (middle, left) or with touch controllers (middle, right). The avatar's hands are varied to fit the participant's hands in size or to be 25\% larger or smaller (right). ABSTRACT Most commercial virtual reality applications with self avatars provide users with a "one-size fits all" avatar. While the height of this body may be scaled to the user's height, other body proportions, such as limb length and hand size, are rarely customized to fit an individual user. Prior research has shown that mismatches between users' avatars and their actual bodies can affect size perception and feelings of body ownership. In this paper, we consider how concepts related to the virtual hand illusion, user experience, and task efficiency are influenced by variations between the size of a user's actual hand and their avatar's hand. We also consider how using a tracked controller or tracked gestures affect these concepts. We conducted a 2x3 within-subjects study (n=20), with two levels of input modality: using tracked finger motion vs. a hand-held controller (Glove vs. Controller), and three levels of hand scaling (Small, Fit, and Large). Participants completed 2 block-assembly trials for each condition (for a total of 12 trials). Time, mistakes, and a user experience survey were recorded for each trial. Participants experienced stronger feelings of ownership and realism in the Glove condition. Efficiency was higher in the Controller condition and supported by play data of more time spent, blocks grabbed, and blocks dropped in the Glove condition. We did not find enough evidence for a change in agency and the intensity of the virtual hand illusion depending on hand size. Over half of the * participants indicated preferring the Glove condition over the Controller condition, mentioning fun and efficiency as factors in their choices. Preferences on hand scaling were mixed but often attributed to efficiency. Participants liked the appearance of their virtual hand more while using the Fit instead of Large hands. Several interaction effects were observed between input modality and hand scaling, for example, for smaller hands, tracked hands evoked stronger feelings of ownership compared to using a controller. Our results show that the virtual hand illusion is stronger when participants are able to control a hand directly rather than with a hand-held device, and that the virtual reality task must first be considered to determine which modality and hand size are the most applicable.},
   author = {Lorraine Lin and Aline Normoyle and Alexandra Adkins and Yu Sun and Andrew Robb and Yuting Ye || and Massimiliano Di Luca and Sophie J ¨ Org},
   keywords = {Computing methodologies-Perception,Human-centered computing-Interaction design,Index Terms: Human-centered computing-Virtual real},
   title = {The Effect of Hand Size and Interaction Modality on the Virtual Hand Illusion},
   url = {https://www.youtube.com/watch?v=Xx21y9eJq1U},
}
@webpage{Beauchamp,
   author = {Daniel Beauchamp},
   title = {Locomotion in VR has just been solved. Pack it in, folks. | Twitter},
   url = {https://twitter.com/pushmatrix/status/1227302127862734849},
}
@article{Koelsch,
   abstract = {Biomechanics determines the physical range in which humans can move their bodies. Human factors research delineates a subspace in which humans can operate without experiencing musculoskeletal strain, fatigue or discomfort. We claim that there is an even tighter space which we call the comfort zone. It is defined as the range of postures adopted voluntarily — despite the availability of other postures. We introduce a measurable, objective foundation for comfort, which was previously assumed equivalent to the absence of discomfort, a subjective quantity. Interfaces designed outside a user's comfort zone can prompt the adoption of alternative use patterns, which are often less favorable because they trade off the unnoticeable potential of injury for comfort. Designing interfaces within the limits of comfort zones can avert these risks.},
   author = {Mathias Kölsch and Andrew C. Beall and Matthew Turk},
   doi = {10.1177/154193120304700413},
   issn = {2169-5067},
   issue = {4},
   journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
   month = {10},
   pages = {725-728},
   publisher = {SAGE Publications},
   title = {An Objective Measure for Postural Comfort},
   volume = {47},
   url = {https://journals.sagepub.com/doi/abs/10.1177/154193120304700413},
   year = {2003},
}
@article{Royden,
   abstract = {When an observer moves through an environment containing stationary and moving objects, he or she must be able to determine which objects are moving relative to the others in order to navigate successfully and avoid collisions. We investigated whether image speed can be used as a cue to detect a moving object in the scene. Our results show that image speed can be used to detect moving objects as long as the object is moving sufficiently faster or slower than it would if it were part of the stationary scene. © 2012 Elsevier Ltd.},
   author = {Constance S. Royden and Kathleen D. Moore},
   doi = {10.1016/j.visres.2012.02.006},
   issn = {00426989},
   journal = {Vision Research},
   keywords = {Heading,Motion,Moving object detection,Optic flow},
   month = {4},
   pages = {17-24},
   pmid = {22406544},
   publisher = {Pergamon},
   title = {Use of speed cues in the detection of moving objects by moving observers},
   volume = {59},
   year = {2012},
}
@article{MartinUsoh1995,
   author = {Mel Slater Martin Usoh and Anthony Steed},
   doi = {10.1145/210079.210084},
   issn = {15577325},
   issue = {3},
   journal = {ACM Transactions on Computer-Human Interaction (TOCHI)},
   pages = {201-219},
   title = {Taking Steps: The Influence of a Walking Technique on Presence in Virtual Reality},
   volume = {2},
   year = {1995},
}
@inproceedings{Bowman,
   abstract = {We present a categorization of techniques for first-person motion control, or travel, through immersive virtual environments, as well as a framework for evaluating the quality of different techniques for specific virtual environment tasks. We conduct three quantitative experiments within this framework: a comparison of different techniques for moving directly to a target object varying in size and distance, a comparison of different techniques for moving relative to a reference object, and a comparison of different motion techniques and their resulting sense of 'disorientation' in the user. Results indicate that 'pointing' techniques are advantageous relative to 'gaze-directed' steering techniques for a relative motion task, and that motion techniques which instantly teleport users to new locations are correlated with increased user disorientation.},
   author = {Doug A. Bowman and David Koller and Larry F. Hodges},
   doi = {10.1109/vrais.1997.583043},
   journal = {Proceedings - Virtual Reality Annual International Symposium},
   pages = {45-52},
   publisher = {IEEE},
   title = {Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques},
   year = {1997},
}
@article{VanBeers1999,
   abstract = {To localize one's hand, i.e., to find out its position with respect to the body, humans may use proprioceptive information or visual information or both. It is still not known how the CNS combines simultaneous proprioceptive and visual information. In this study, we investigate in what position in a horizontal plane a hand is localized on the basis of simultaneous proprioceptive and visual information and compare this to the positions in which it is localized on the basis of proprioception only and vision only. Seated at a table, subjects matched target positions on the table top with their unseen left hand under the table. The experiment consisted of three series. In each of these series, the target positions were presented in three conditions: by vision only, by proprioception only, or by both vision and proprioception. In one of the three series, the visual information was veridical. In the other two, it was modified by prisms that displaced the visual field to the left and to the right, respectively. The results show that the mean of the positions indicated in the condition with both vision and proprioception generally lies off the straight line through the means of the other two conditions. In most cases the mean lies on the side predicted by a model describing the integration of multisensory information. According to this model, the visual information and the proprioceptive information are weighted with direction-dependent weights, the weights being related to the direction-dependent precision of the information in such a way that the available information is used very efficiently. Because the proposed model also can explain the unexpectedly small sizes of the variable errors in the localization of a seen hand that were reported earlier, there is strong evidence to support this model. The results imply that the CNS has knowledge about the direction-dependent precision of the proprioceptive and visual information.},
   author = {Robert J. Van Beers and Anne C. Sittig and Jan J. Denier Van Der Gon},
   doi = {10.1152/jn.1999.81.3.1355},
   issn = {00223077},
   issue = {3},
   journal = {Journal of Neurophysiology},
   keywords = {A C Sittig,Adult,Female,Humans,J J Gon,MEDLINE,Male,Mental Processes / physiology*,Middle Aged,Models,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Neurological*,Pattern Recognition,Proprioception / physiology*,PubMed Abstract,R J van Beers,Visual / physiology*,doi:10.1152/jn.1999.81.3.1355,pmid:10085361},
   pages = {1355-1364},
   pmid = {10085361},
   publisher = {American Physiological Society},
   title = {Integration of proprioceptive and visual position-information: An experimentally supported model},
   volume = {81},
   url = {https://pubmed.ncbi.nlm.nih.gov/10085361/},
   year = {1999},
}
@report{Bubka,
   author = {Andrea Bubka and Frederick Bonato and Stephen Palmisano},
   title = {Expanding and contracting optical flow patterns and simulator sickness Effects of postural constraints upon saccadic eye movements View project ERP Markers of Auditory Go/NoGo Processing View project},
   url = {https://www.researchgate.net/publication/6347727},
}
@book{Gibson,
   abstract = {The principal subject of this book is the visual perception of space. Chapters cover theories of perception, the visual field and visual world, formation of retinal images, a psychophysical theory of perception, stimulus variables for visual depth and distance, size and shape constancy, geometrical space and form, mean, learning, and spatial behavior.},
   author = {James J Gibson},
   title = {The perception of the visual world},
   year = {1950},
}
@inproceedings{Sarupuri2017,
   abstract = {We present Trigger Walking, a low-fatigue travel technique for immersive virtual reality which uses hand-held controllers to move about more naturally within a limited physical space. Most commercial applications use some form of teleportation or physical walking for moving around in a virtual space. However, teleportation can be disorienting, due to the sudden change in the environment when teleported to another location. Physical walking techniques are more physically demanding, leading to fatigue. Hence, we explore the use of two spatial controllers that accompany commercial headsets to walk by taking a virtual step each time a controller trigger is pulled. The user has the choice of using the orientation of a single-controller, the average of both controllers, or that of the head to determine the direction of walking, and speed can be controlled by changing the angle of the controller to the Frontal plane.},
   author = {Bhuvaneswari Sarupuri and Miriam Luque Chipana and Robert W. Lindeman},
   doi = {10.1109/3DUI.2017.7893354},
   isbn = {9781509067169},
   journal = {2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings},
   keywords = {HMD,Tracking,entertainment,immersion,training},
   month = {4},
   pages = {227-228},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Trigger Walking: A low-fatigue travel technique for immersive virtual reality},
   year = {2017},
}
@inproceedings{Yan2016,
   abstract = {The tradeoff between speed and precision is one of the challenging problems of travel interfaces. Sometimes users want to travel long distances (e.g., fly) and care less about precise movement, while other times they want to approach nearby objects in a more-precise way (e.g., walk), and care less about how quickly they move. Between these two extremes there are scenarios when both speed and precision become equally important. In real life, we often seamlessly combine these modes. However, most VR systems support a single travel metaphor, which may only be good for one range of travel, but not others. We present a new VR travel framework which supports three separate multi-touch travel techniques, one for each distance range, but that all use the same device. We use a unifying metaphor of the user's fingers becoming their legs for each of the techniques. We are investigating the usability and user acceptance of the fingers-as-legs metaphor, as well as the efficiency and naturalness of switching between the different travel modes. We conducted an experiment focusing on user performance using the three travel modes, and compared our multi-touch, gesture-based approach with a traditional Gamepad travel interface. The results suggest that participants using a Gamepad interface are more time efficient. However, the quality of completing the tasks with the two input devices was similar, while ForcePad user response was faster for switching between travel modes.},
   author = {Zhixin Yan and Robert W. Lindeman and Arindam Dey},
   doi = {10.1109/3DUI.2016.7460027},
   isbn = {9781509008421},
   journal = {2016 IEEE Symposium on 3D User Interfaces, 3DUI 2016 - Proceedings},
   keywords = {3D travel interface,multi-touch gestures},
   month = {4},
   pages = {27-30},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Let your fingers do the walking: A unified approach for efficient short-, medium-, and long-distance travel in VR},
   year = {2016},
}
@inproceedings{Griffin2018,
   abstract = {To navigate beyond the confines of often limited available positional tracking space, virtual reality (VR) users need to switch from natural walking input to a controller-based locomotion technique, such as teleportation or full locomotion. Overloading the hands with navigation functionality has been considered detrimental to performance given that, in many VR experiences, (i.e., games), controllers are already used for tasks such as shooting or interacting with objects. Existing studies have only evaluated virtual locomotion techniques using a single navigation task. This paper reports on the performance, cognitive load demands, usability, presence and VR sickness occurrence of two hands-busy (full locomotion/teleportation) and two hands-free (tilt/walking-in-place) locomotion methods while participants (n=20) performed a bimanual shooting with navigation task. Though handsfree methods offer a higher presence, they don’t outperform handsbusy locomotion methods in terms of performance.},
   author = {Nathan Navarro Griffin and James Liu and Eelke Folmer},
   doi = {10.1145/3242671.3242707},
   isbn = {9781450356244},
   journal = {CHI PLAY 2018 - Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play},
   keywords = {Bimanual performance,Cognitive load,Full locomotion,Teleportation,Virtual reality; Locomotion},
   month = {10},
   pages = {211-219},
   publisher = {Association for Computing Machinery, Inc},
   title = {Evaluation of handsbusy vs handsfree virtual locomotion},
   url = {http://dx.doi.org/10.1145/3242671.3242707},
   year = {2018},
}
@inproceedings{Forte2019,
   abstract = {In recent years there has been a great boom in the use of immersive virtual environments applications, but research into interaction techniques for these technologies has not had the same growth. Therefore, it is necessary to study the user experience of the different forms of interaction that these technologies offer to users and give developers the information needed to use the techniques that best suit their applications and their users. This work is an exploratory study to detect the problems that the users find in this kind of applications. We aim to make an evaluation of a determined set of interaction techniques, both in virtual and augmented reality, attending to the problems produced in the users to know the strengths and weaknesses of each type of interaction technique. With this study, we will be able to make a guide for the developers, in order to give them clues about the best interaction technique for their applications.},
   author = {Juan Luis Berenguel Forte and Francisco Luis Gutiérrez Vela and Patricia Paderewski Rodríguez},
   doi = {10.1145/3335595.3336288},
   isbn = {9781450371766},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Augmented reality,Evaluation,Immersive virtual environments,Interaction techniques,Usability,User experience,Virtual reality},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {User experience problems in immersive virtual environments},
   url = {https://doi.org/10.1145/3335595.3336288},
   year = {2019},
}
@report{Bhandari,
   abstract = {Figure 1: Teleportation discontinuously translates the user's viewpoint over a distance (A → B). The absence of optical flow reduces VR sickness, but also limits the users' ability to perform path integration, i.e., estimating the distance traveled, which can lead to spatial disorientation. Dash merges teleportation with regular locomotion by quickly and continuously moving the user to a destination (A → C), which generates optical flow that allows for path integration. ABSTRACT Teleportation is a popular locomotion technique that lets users navigate beyond the confines of limited available positional tracking space. Because it discontinuously translates the viewpoint, it is considered a safe locomotion method because it doesn't generate any optical flow, and thus reduces the risk of vection induced VR sickness. Though the lack of optical flow minimizes VR sickness, it also limits path integration, e.g., estimating the total distance traveled, and which can lead to spatial disorientation. This paper evaluates a teleportation technique called Dash that quickly but continuously displaces the user's viewpoint and which retains some optical flow cues. A user study with 16 participants compares Dash to regular teleportation and found that it significantly improves path integration while there was no difference in VR sickness.},
   author = {Jiwan Bhandari and Paul Macneilage and Eelke Folmer},
   keywords = {Teleportation,VR Sickness Index Terms: I37 [Computer Graphics]: 3D Graphics and Realism-Virtual Reality,Virtual Locomotion},
   title = {Teleportation without Spatial Disorientation Using Optical Flow Cues},
   year = {2018},
}
@inproceedings{Clifton,
   abstract = {We compared two common techniques of controller-based locomotion (teleportation and steering locomotion) in virtual reality (VR) in terms of the cybersickness they produce. Participants had to continuously navigate a commercial VR application for 16 minutes using each technique, while standing and seated. While teleportation produced less cybersickness than steering locomotion on average, a number of participants reported teleportation to be more sickening. These 'telesick' participants were found to have greater medio/lateral positional variability in their spontaneous postural sway than 'steersick' participants prior to VR exposure. We conclude that different individuals may require unique techniques to comfortably locomote in VR.},
   author = {Jeremy Clifton and Stephen Palmisano},
   doi = {10.1145/3359996.3364722},
   isbn = {9781450370011},
   journal = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
   keywords = {Cybersickness,Head-Mounted Display,Locomotion,Virtual Reality},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Comfortable locomotion in VR: Teleportation is not a complete solution},
   year = {2019},
}
@inproceedings{Cardoso2016,
   abstract = {In this paper we present a VR locomotion technique based on the Leap Motion device and compare it to other often-used locomotion techniques - gaze-directed locomotion and gamepad-based locomotion. We performed a user experiment to evaluate the three techniques based on their performance (time to complete the task), comfort (through the ISO 9241-9 assessment of comfort questionnaire), and simulation sickness (through the Simulation Sickness Questionnaire). Results indicate that the gamepad technique is both faster and more comfortable than either the Leap Motion-based or the gaze-directed techniques.},
   author = {Jorge C.S. Cardoso},
   doi = {10.1145/2993369.2996327},
   journal = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
   title = {Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds},
   volume = {02-04-Nove},
   year = {2016},
}
@inproceedings{Cardoso2017,
   abstract = {In this paper, we present a VR locomotion technique based on the Leap Motion device and compare it to other often-used locomotion techniques - gaze-directed locomotion and gamepad-based locomotion. We performed an exploratory user experiment to evaluate the three techniques based on their performance (time to complete the task), comfort (through the ISO 9241-9 assessment of comfort questionnaire), and simulation sickness (through the Simulation Sickness Questionnaire). Results indicate that the gamepad technique is both faster and more comfortable than either the Leap Motion-based or the gaze-directed techniques. These results suggest that the design of interaction techniques for the Leap Motion device should take into consideration the possible fatigue induced by the prolonged use of the device. For interaction tasks that require high performance, designers should look for alternatives to the Leap Motion device.},
   author = {Jorge C.S. Cardoso},
   journal = {Proceedings of the International Conference on Interfaces and Human Computer Interaction 2017 - Part of the Multi Conference on Computer Science and Information Systems 2017},
   title = {Gesture-based locomotion in immersive VR worlds with the leap motion controller},
   year = {2017},
}
@inproceedings{JacobHabgood2018,
   abstract = {The confounding effect of player locomotion on the vestibulo-ocular reflex is one of the principal causes of motion sickness in immersive virtual reality. Continuous motion is particularly problematic for stationary user configurations, and teleportation has become the prevailing approach for providing accessible locomotion. Unfortunately, teleportation can also increase disorientation and reduce a player's sense of presence within a VR environment. This paper presents an alternative locomotion technique designed to preserve accessibility while maintaining feelings of presence. This is a node-based navigation system which allows the player to move between predefined node positions using a rapid, continuous, linear motion. An evaluation was undertaken to compare this locomotion technique with commonly used, teleportation-based and continuous walking approaches. Thirty-six participants took part in a study which examined motion sickness and presence for each technique, while navigating around a virtual house using PlayStation VR. Contrary to intuition, we show that rapid movement speeds reduce players' feelings of motion sickness as compared to continuous movement at normal walking speeds.},
   author = {M. P. Jacob Habgood and David Moore and David Wilson and Sergio Alapont},
   doi = {10.1109/VR.2018.8446130},
   isbn = {9781538633656},
   journal = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
   keywords = {Edward Jenner,PlayStation VR,REVEAL.: H.5.1 [Information Interfaces and Present,and virtual realities,augmented,cultural heritage,locomotion,motion-sickness,virtual reality},
   month = {8},
   pages = {371-378},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Rapid, Continuous Movement between Nodes as an Accessible Virtual Reality Locomotion Technique},
   year = {2018},
}
@inproceedings{Frommel2017,
   abstract = {Entertainment and in particular gaming is currently considered one of the main application scenarios for virtual reality (VR). The majority of current games rely on any form of locomotion through the virtual environment while some techniques can lead to simulator sickness. Game developers are currently implementing a wide variety of locomotion techniques to cope with simulator sickness (e.g. teleportation). In this work we implemented and evaluated four diierent controller-based locomotion methods that are popular in current VR games (free teleport, ,xpoint teleport, touchpad-based, automatic). We conducted a user study (n = 24) in which participants explored a virtual zoo with these four diierent controller-based locomotion methods and assessed their eeects on discomfort, presence, enjoyment, and aaective state. The results of our study show that free teleport locomotion elicited least discomfort and provided the highest scores for enjoyment, presence, and aaective state. With these results we gained valuable insights for developers and researchers implementing grst person locomotion in VR experiences.},
   author = {Julian Frommel and Sven Sonntag and Michael Weber},
   city = {New York, NY, USA},
   isbn = {9781450353199},
   journal = {Proceedings of the 12th International Conference on the Foundations of Digital Games},
   keywords = {•Applied computing  Computer games,•Human-centered computing  Virtual reality,•Software and its engineering  Interactive games},
   publisher = {ACM},
   title = {EEects of Controller-based Locomotion on Player Experience in a Virtual Reality Exploration Game},
   volume = {6},
   year = {2017},
}
@article{Boletsis,
   abstract = {The latest technical and interaction advancements within the virtual reality (VR) field have marked a new era, not only for VR, but also for VR locomotion. In this era, well-established, prevalent VR locomotion techniques are mostly used as points of comparison for benchmarking of new VR locomotion designs. At the same time, there is the need for more exploratory, comparative studies of contemporary VR locomotion techniques, so that their distinguished interaction aspects can be documented and guide the design process of new techniques. This article presents a comparative, empirical evaluation study of contemporary and prevalent VR locomotion techniques, examining the user experience (UX) they offer. First, the prevalent VR locomotion techniques are identified based on literature, i.e., walking-in-place, controller/joystick, and teleportation. Twenty-six adults are enrolled in the study and perform a game-like task using the techniques. The study follows a mixed methods approach, utilising the System Usability Scale survey, the Game Experience Questionnaire, and a semistructured interview to assess user experiences. Results indicate that the walking-in-place technique offers the highest immersion but also presents high levels of psychophysical discomfort. Controller/joystick VR locomotion is perceived as easy-to-use due to the users' familiarity with controllers, whereas teleportation is considered to be effective due to its fast navigation, although its visual 'jumps' do break the users' sense of immersion. Based on the interviews, the users focused on the following interaction dimensions to describe their VR locomotion experiences: (i) immersion and flow, (ii) ease-of-use and mastering, (iii) competence and sense of effectiveness, and (iv) psychophysical discomfort. The study implications for VR locomotion are discussed, along with the study limitations and the future direction for research.},
   author = {Costas Boletsis and Jarl Erik Cedergren},
   doi = {10.1155/2019/7420781},
   issn = {16875907},
   journal = {Advances in Human-Computer Interaction},
   publisher = {Hindawi Limited},
   title = {VR Locomotion in the New Era of Virtual Reality: An Empirical Comparison of Prevalent Techniques},
   volume = {2019},
   year = {2019},
}
@report{Bouguila,
   abstract = {Omni-directional locomotion systems are yet of little advantage in virtual environments (VEs) with limited large display system, where users may experience visual-less situations when they move in a direction that is not covered by the large screen. This paper presents a new omni-directional locomotion interface based on step-in-place movement and a smart-turntable system to impart users with the ability to move freely in any direction within VEs without loosing sight of the displayed images despite their projection on a limited large screen that do not provide surrounding or 360 o visual feedback. A sensor-embedded turntable is used as a walking platform, on top of which users will stand at its center to perform walk in place and turn in place movements to steer their navigation through the virtual environment. However, as a large turn action may put the screen outside user's visual field of view, the turntable will cancel user's turnings by a smooth and passive rotation in the opposite direction so as to keep user oriented toward the center of the screen. The novelty of the interface is that a) it uses a smart-turntable as walking platform that compensate users' rotations rather than their displacements b) no cable attachments are made to the user body c) user can make many full body rotations without loosing sight of the environment, virtually providing a surrounding display despite the use of limited size screen.},
   author = {Laroussi Bouguila and Masahiro Ishii and Makoto Sato},
   title = {Realizing a New Step-in-place Locomotion interface for Virtual Environment with Large Display System},
}
@inproceedings{Ferracani,
   abstract = {In this paper we evaluate methods to move 'naturally' in an Immersive Virtual Environment (IVE) visualised through an Head Mounted Display (HMD). Natural interaction is provided through gesture recognition on depth sensors' data. Gestural input solutions in the literature to provide loco-motion are discussed. Two new methods for locomotion are proposed, implemented in a framework used for comparative evaluation. Perceived naturalness and effectiveness of locomotion methods are assessed through qualitative and quantitative measures. Extensive tests are conducted on the locomotion considering also: 1) obstacles in navigation; 2) interaction with virtual objects during locomotion. This is done with the aim to identify methods capable to provide a full body experience in an IVE. Results show that one of the methods for locomotion we propose has a performance comparable to established techniques in literature. Outcomes may be exploited to improve the naturalness of users' movements in IVEs and help to unlock new strategies in providing IVEs for learning, training, collaboration and entertainment, also with respect to users with disabilities.},
   author = {Andrea Ferracani and Daniele Pezzatini and Jacopo Bianchini and Gianmarco Biscini and Alberto Del Bimbo},
   city = {New York, NY, USA},
   isbn = {9781450345217},
   journal = {Proceedings of the 1st International Workshop on Multimedia Alternate Realities},
   keywords = {Head Mounted Display,Immersive Virtual Reality,Locomotion,Motion Tracking,Natural Interac-tion},
   publisher = {ACM},
   title = {Locomotion by Natural Gestures for Immersive Virtual Environments},
   url = {http://dx.doi.org/10.1145/2983298.2983307},
}
@inproceedings{Langbehn,
   abstract = {Due to its multimodal nature virtual reality technology imposes new challenges, for example, when it comes to navigating through a virtual environment. Joystick-based controls and teleportation techniques support only limited self-motion experiences, however, other techniques such as redirected walking provide promising solutions to enable near-natural walking, while overcoming limits of the physical space. In this article, we report an experiment that analyzed the effects of the three different locomotion techniques, i. e., (i) joystick-based, (ii) teleportation, and (iii) redirected walking, on the user's cognitive map building of an indoor virtual environment , as well as effectiveness, motion sickness, presence, and user preferences. Our results suggest that redirected walking performs best regarding the user's ability to unconsciously acquire spatial knowledge about the virtual environment. Redirected walking and teleportation were subjectively preferred over joystick by the participants. Furthermore, we found a significant effect of an increased motion sickness for joystick-based navigation. Hence, redirected walking as well as teleportation are locomotion techniques with different benefits and drawbacks, and should be preferred.},
   author = {Eike Langbehn and Paul Lubos and Frank Steinicke},
   city = {New York, NY, USA},
   isbn = {9781450353816},
   issue = {18},
   journal = {Proceedings of the Virtual Reality International Conference - Laval Virtual},
   keywords = {Virtual reality,cognitive map building,locomotion,navigation,redirected walking,spatial knowledge,teleportation},
   publisher = {ACM},
   title = {Evaluation of Locomotion Techniques for Room-Scale VR: Joystick, Teleportation, and Redirected Walking},
   url = {https://doi.org/10.1145/3234253.3234291},
}
@inproceedings{Nabiyouni2015,
   abstract = {One of the goals of much virtual reality (VR) research is to increase realism. In particular, many techniques for locomotion in VR attempt to approximate real-world walking. However, it is not yet fully understood how the design of more realistic locomotion techniques affects user task performance. We performed an experiment to compare a semi-natural locomotion technique (based on the Virtusphere device) with a traditional, non-natural technique (based on a game controller) and a fully natural technique (real walking). We found that the Virtusphere technique was significantly slower and less accurate than both of the other techniques. Based on this result and others in the literature, we speculate that locomotion techniques with moderate interaction fidelity will often have performance inferior to both high-fidelity techniques and well-designed low-fidelity techniques. We argue that our experimental results are an effect of interaction fidelity, and perform a detailed analysis of the fidelity of the three locomotion techniques to support this argument.},
   author = {Mahdi Nabiyouni and Ayshwarya Saktheeswaran and Doug A. Bowman and Ambika Karanth},
   doi = {10.1109/3DUI.2015.7131717},
   isbn = {9781467368865},
   journal = {2015 IEEE Symposium on 3D User Interfaces, 3DUI 2015 - Proceedings},
   keywords = {Effectiveness,Interaction fidelity,Locomotion interaction,Virtusphere},
   month = {6},
   pages = {3-10},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Comparing the performance of natural, semi-natural, and non-natural locomotion techniques in virtual reality},
   year = {2015},
}
@inproceedings{Bozgeyikli2016,
   abstract = {With the increasing popularity of virtual reality (VR) and new devices getting available with relatively lower costs, more and more video games have been developed recently. Most of these games use first person interaction techniques since it is more natural for Head Mounted Displays (HMDs). One of the most widely used interaction technique in VR video games is locomotion that is used to move user's viewpoint in virtual environments. Locomotion is an important component of video games since it can have a strong influence on user experience. In this study, a new locomotion technique we called "Point \& Teleport" is described and compared with two commonly used VR locomotion techniques of walk-in-place and joystick. In this technique, users simply point where they want to be in virtual world and they are teleported to that position. As a major advantage, it is not expected to introduce motion sickness since it does not involve any visible translational motion. In this study, two VR experiments were designed and performed to analyze the Point \& Teleport technique. In the first experiment, Point \& Teleport was compared with walkin-place and joystick locomotion techniques. In the second experiment, a direction component was added to the Point \& Teleport technique so that the users could specify their desired orientation as well. 16 users took part in both experiments. Results indicated that Point \& Teleport is a fun and user friendly locomotion method whereas the additional direction component degraded the user experience.},
   author = {Evren Bozgeyikli and Andrew Raij and Srinivas Katkoori and Rajiv Dubey},
   doi = {10.1145/2967934.2968105},
   isbn = {9781450344562},
   journal = {CHI PLAY 2016 - Proceedings of the 2016 Annual Symposium on Computer-Human Interaction in Play},
   keywords = {Locomotion,Teleportation,Virtual reality},
   month = {10},
   pages = {205-216},
   publisher = {Association for Computing Machinery, Inc},
   title = {Point \& Teleport locomotion technique for virtual reality},
   url = {http://dx.doi.org/10.1145/2967934.2968105},
   year = {2016},
}
@generic{Boletsis2017,
   abstract = {The latest technical and interaction advancements that took place in the Virtual Reality (VR) field have marked a new era, not only for VR, but also for VR locomotion. Although the latest advancements in VR locomotion have raised the interest of both researchers and users in analyzing and experiencing current VR locomotion techniques, the field of research on VR locomotion, in its new era, is still uncharted. In this work, VR locomotion is explored through a systematic literature review investigating empirical studies of VR locomotion techniques from 2014–2017. The review analyzes the VR locomotion techniques that have been studied, their interaction-related characteristics and the research topics that were addressed in these studies. Thirty-six articles were identified as relevant to the literature review, and the analysis of the articles resulted in 73 instances of 11 VR locomotion techniques, such as real-walking, walking-in-place, point and teleport, joystick-based locomotion, and more. Results showed that since the VR revival, the focus of VR locomotion research has been on VR technology and various technological aspects, overshadowing the investigation of user experience. From an interaction perspective, the majority of the utilized and studied VR locomotion techniques were found to be based on physical interaction, exploiting physical motion cues for navigation in VR environments. A significant contribution of the literature review lies in the proposed typology for VR locomotion, introducing four distinct VR locomotion types: motion-based, room scale-based, controller-based and teleportation-based locomotion.},
   author = {Costas Boletsis},
   doi = {10.3390/mti1040024},
   issn = {24144088},
   issue = {4},
   journal = {Multimodal Technologies and Interaction},
   keywords = {Human-computer interaction,Literature review,Locomotion,Typology,Virtual reality},
   month = {12},
   pages = {24},
   publisher = {MDPI AG},
   title = {The new era of virtual reality locomotion: A systematic literature review of techniques and a proposed typology},
   volume = {1},
   url = {www.mdpi.com/journal/mti},
   year = {2017},
}
@article{Smith,
   abstract = {Fig. 1. A subject brings her hands together, bends her middle fingers, pivots her hands around this region of contact, intertwines her remaining fingers, and wiggles her middle fingers. Top row: Input images. Bottom row: Our tracking results. Our approach is able to track through the significant amount of self-contact and and self-occlusion induced by this two-handed performance. Many of the actions that we take with our hands involve self-contact and occlusion: shaking hands, making a fist, or interlacing our fingers while thinking. This use of of our hands illustrates the importance of tracking hands through self-contact and occlusion for many applications in computer vision and graphics, but existing methods for tracking hands and faces are not designed to treat the extreme amounts of self-contact and self-occlusion exhibited by common hand gestures. By extending recent advances in vision-based tracking and physically based animation, we present the first algorithm capable of tracking high-fidelity hand deformations through highly self-contacting and self-occluding hand gestures, for both single hands and two hands. By constraining a vision-based tracking algorithm with a physically based deformable model, we obtain an algorithm that is robust to the ubiquitous self-interactions and massive self-occlusions exhibited by common hand gestures, allowing us to track two hand interactions and some of the most difficult possible configurations of a human hand.},
   author = {Breannan Smith and Jessica K Hodgins and • Breannan Smith and Chenglei Wu and He Wen and Patrick Peluse and Yaser Sheikh and Takaaki Shiratori},
   doi = {10.1145/3414685.3417768},
   issue = {6},
   journal = {ACM Trans. Graph},
   keywords = {Authors' addresses: Breannan Smith, Facebook Reality Labs Research, breannan@ fbcom,Chenglei Wu, Facebook Reality Labs Research, chenglei@fbcom,He Wen, Facebook Reality Labs Research, hewen@fbcom,Jessica K Hodgins, Facebook AI Research, jkh@fbcom,Patrick Peluse, Facebook Reality Labs Research, ppeluse@fbcom,Takaaki Shiratori, CCS Concepts: • Computing methodologies → Motion capture Additional Key Words and Phrases: hand tracking, simulation, elasticity,Yaser Sheikh, Facebook Reality Labs Research, yasers@ fbcom},
   title = {Constraining Dense Hand Surface Tracking with Elasticity},
   volume = {39},
   url = {https://doi.org/10.1145/3414685.3417768},
}
@article{Han,
   abstract = {Fig. 1. We present a real-time hand-tracking system using four monochrome cameras mounted on a VR headset. We output the user's skeletal poses and rigged hand model meshes. Here we show some snapshots of users using our system to drive interactive VR experiences. We present a system for real-time hand-tracking to drive virtual and augmented reality (VR/AR) experiences. Using four fisheye monochrome cameras , our system generates accurate and low-jitter 3D hand motion across a large working volume for a diverse set of users. We achieve this by proposing neural network architectures for detecting hands and estimating hand key-point locations. Our hand detection network robustly handles a variety of real world environments. The keypoint estimation network leverages tracking history to produce spatially and temporally consistent poses. We design scalable, semi-automated mechanisms to collect a large and diverse set of ground truth data using a combination of manual annotation and automated tracking. Additionally, we introduce a detection-by-tracking method that increases smoothness while reducing the computational cost; the optimized system runs at 60Hz on PC and 30Hz on a mobile processor. Together, these contributions yield a practical system for capturing a user's hands and is the default feature on the Oculus Quest VR headset powering input and social presence.},
   author = {Shangchen Han and Beibei Liu and Randi Cabezas and Christopher D Twigg and Peizhao Zhang and Jeff Petkau and Tsz-Ho Yu and Chun-Jung Tai and Muzaffer Akbay and Zheng Wang and Asaf Nitzan and Gang Dong and Yuting Ye and Lingling Tao and Chengde Wan and Robert Wang},
   doi = {10.1145/3386569.3392452},
   journal = {ACM Trans. Graph. 39, 4, Article},
   keywords = {Asaf Nitzan, asafn@fbcom,CCS Concepts: • Computing methodologies → Computer vision Additional Key Words and Phrases: motion capture, hand tracking, virtual reality ACM Reference Format:,Chengde Wan, vgrasp@fbcom,Gang Dong, gangdong@fbcom,Lingling Tao, linglingt@fbcom,Robert Wang, rywang@fbcom, Facebook Reality Labs,Yuting Ye, yutingye@fb com},
   pages = {13},
   title = {MEgATrack: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality},
   volume = {1},
   url = {https://doi.org/10.1145/3386569.3392452},
   year = {2020},
}
@report{Moon,
   abstract = {Analysis of hand-hand interactions is a crucial step towards better understanding human behavior. However, most researches in 3D hand pose estimation have focused on the isolated single hand case. Therefore, we firstly propose (1) a large-scale dataset, InterHand2.6M, and (2) a baseline network, InterNet, for 3D interacting hand pose estimation from a single RGB image. The proposed InterHand2.6M consists of 2.6M labeled single and interacting hand frames under various poses from multiple subjects. Our InterNet simultaneously performs 3D single and interacting hand pose estimation. In our experiments, we demonstrate big gains in 3D interacting hand pose estimation accuracy when leveraging the interacting hand data in InterHand2.6M. We also report the accuracy of InterNet on InterHand2.6M, which serves as a strong baseline for this new dataset. Finally, we show 3D interacting hand pose estimation results from general images. Our code and dataset are available 1 .},
   author = {Gyeongsik Moon and Shoou-I Yu and He Wen and Takaaki Shiratori and Kyoung Mu Lee},
   title = {InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image},
   url = {https://mks0601.github.io/InterHand2.6M/},
}
@article{Luca,
   abstract = {Figure 1: Features of the Locomotion Vault interactive database and visualization include: filtering by attributes (left image), an animated gallery with individual technique descriptions for over 100 locomotion techniques (middle), and two similarity graphs that are expert-created or calculated from the attributes (right). ABSTRACT Numerous techniques have been proposed for locomotion in virtual reality (VR). Several taxonomies consider a large number of attributes (e.g., hardware, accessibility) to characterize these techniques. However, finding the appropriate locomotion technique (LT) and identifying gaps for future designs in the high-dimensional space of attributes can be quite challenging. To aid analysis and innovation, we devised Locomotion Vault (https://locomotionvault. github.io/), a database and visualization of over 100 LTs from academia and industry. We propose similarity between LTs as a metric to aid navigation and visualization. We show that similarity based on attribute values correlates with expert similarity assessments (a method that does not scale). Our analysis also highlights an inherent trade-off between simulation sickness and accessibility across LTs. As such, Locomotion Vault shows to be a tool that unifies information on LTs and enables their standardization and large-scale comparison to help understand the space of possibilities in VR locomotion. CCS CONCEPTS • Human-centered computing → User interface management systems; Information visualization.},
   author = {Massimiliano Di Luca and Hasti Seifi and Simon Egan and Mar Gonzalez-Franco},
   doi = {10.1145/3411764.3445319},
   isbn = {978-1-4503-8096-6},
   keywords = {VR,database,locomotion method,locomotion technique,movement,navigation,travel-ing,visualization},
   title = {Locomotion Vault: the Extra Mile in Analyzing VR Locomotion Tech-niques},
   url = {https://doi.org/10.1145/3411764.3445319},
}
@article{Huang,
   abstract = {Background Within a virtual environment (VE) the control of locomotion (e.g., self-travel) is critical for creating a realistic and functional experience. Usually the direction of locomotion, while using a head-mounted display (HMD), is determined by the direction the head is pointing and the forward or backward motion is controlled with a hand held controllers. However, hand held devices can be difficult to use while the eyes are covered with a HMD. Free hand gestures, that are tracked with a camera or a hand data glove, have an advantage of eliminating the need to look at the hand controller but the design of hand or finger gestures for this purpose has not been well developed. Methods This study used a depth-sensing camera to track fingertip location (curling and straightening the fingers), which was converted to forward or backward self-travel in the VE. Fingertip position was converted to self-travel velocity using a mapping function with three parameters: a region of zero velocity (dead zone) around the relaxed hand position, a linear relationship of fingertip position to velocity (slope or/J) beginning at the edge of the dead zone, and an exponential relationship rather than a linear one mapping fingertip position to velocity (exponent). Using a HMD, participants moved forward along a virtual road and stopped at a target on the road by controlling self-travel velocity with finger flexion and extension. Each of the 3 mapping function parameters was tested at 3 levels. Outcomes measured included usability ratings, fatigue, nausea, and time to complete the tasks. Results Twenty subjects participated but five did not complete the study due to nausea. The size of the dead zone had little effect on performance or usability. Subjects preferred lower β values which were associated with better subjective ratings of control and reduced time to complete the task, especially for large targets. Exponent values of 1.0 or greater were preferred and reduced the time to complete the task, especially for small targets. Conclusions Small finger movements can be used to control velocity of self-travel in VE. The functions used for converting fingertip position to movement velocity influence usability and performance.},
   author = {Rachel HUANG and Carisa HARRIS-ADAMSON and Dan ODELL and David REMPEL},
   doi = {10.3724/sp.j.2096-5796.2018.0007},
   issn = {20965796},
   issue = {1},
   journal = {Virtual Reality \& Intelligent Hardware},
   month = {2},
   pages = {1-9},
   publisher = {Elsevier BV},
   title = {Design of finger gestures for locomotion in virtual reality},
   volume = {1},
   year = {2019},
}
@inproceedings{Davis,
   abstract = {The uptake of new interface technologies, such as the Oculus Rift have generated renewed interest in virtual reality especially for private entertainment use. However, long standing issues with unwanted side effects, such as nausea from cybersickness, continue to impact on the general use of devices such as head mounted displays. This in turn has slowed the uptake of more immersive interfaces for computer gaming and indeed more serious applications in training and health. In this paper we report a systematic review in the area of cybersickness with a focus on measuring the diverse symptoms experienced. Indeed the related conditions of simulator sickness and motion sickness have previously been well studied and yet many of the issues are unresolved. Here we report on these issues along with a number of measures, both subjective and objective in nature, using either questionnaires or psychophysiological measures that have been used to study cybersickness. We also report on the factors, individual, device related and task dependent that impact on the condition. We conclude that there remains a need to develop more cost-effective and objective physiological measures of both the impact of cybersickness and a person's susceptibility to the condition.},
   author = {Simon Davis and Keith Nesbitt and Eugene Nalivaiko},
   city = {New York, NY, USA},
   isbn = {9781450327909},
   journal = {Proceedings of the 2014 Conference on Interactive Entertainment},
   keywords = {Artificial,Ergonomics General Terms Design,H51 Multimedia Information Systems,Human Factors Keywords Cybersickness,Oculus Rift,and virtual realities H52 User Interfaces,augmented,motion sickness,simulator sickness},
   publisher = {ACM},
   title = {A Systematic Review of Cybersickness},
   url = {http://dx.doi.org/10.1145/2677758.2677780},
   year = {2014},
}
@thesis{Stone,
   abstract = {Stone Iii, William B., "Psychometric evaluation of the Simulator Sickness Questionnaire as a measure of cybersickness" (2017). Graduate Theses and Dissertations. 15429.},
   author = {William B. Stone Iii},
   city = {Ames},
   doi = {10.31274/etd-180810-5050},
   institution = {Iowa State University, Digital Repository},
   journal = {Graduate Theses and Dissertations},
   keywords = {Cybersickness,Psychometrics,Simulator sickness,Video games,Virtual reality,Visually-induced motion sickness},
   month = {1},
   title = {Psychometric evaluation of the Simulator Sickness Questionnaire as a measure of cybersickness},
   url = {https://lib.dr.iastate.edu/etd/15429/},
   year = {2017},
}
@inproceedings{Stanney,
   abstract = {Factor analysis of a large number of motion sickness self-reports from exposure to military flight simulators revealed three separate clusters of symptoms. Based on this analysis a symptom profile emerged for simulators where Oculomotor symptoms predominated, followed by Nausea and least by Disorientation-like symptoms. Current users of virtual environment (VE) systems have also begun to report varying degrees of what they are calling cybersickness, which initially appeared to be similar to simulator sickness. We have found, after examination of eight experiments using different VE systems, that the profile of cybersickness is sufficiently different from simulator sickness - with Disorientation being the predominant symptom and Oculomotor the least. The total severity of cybersickness was also found to be approximately three times greater than that of simulator sickness. Perhaps these different strains of motion sickness may provide insight into the different causes of the two maladies.},
   author = {Kay M. Stanney and Robert S. Kennedy and Julie M. Drexler},
   doi = {10.1177/107118139704100292},
   issn = {10711813},
   journal = {Proceedings of the Human Factors and Ergonomics Society},
   month = {11},
   pages = {1138-1141},
   publisher = {Human Factors and Ergonomics Society, Inc.},
   title = {Cybersickness is not simulator sickness},
   volume = {2},
   url = {https://journals.sagepub.com/doi/10.1177/107118139704100292},
   year = {1997},
}
@article{LaViola,
   abstract = {An important and troublesome problem with current virtual environment (VE) technology is the tendency for some users to exhibit symptoms that parallel symptoms of classical motion sickness both during and after the VE experience. This type of sickness, cybersickness, is distinct from motion sickness in that the user is often stationary but has a compelling sense of self motion through moving visual imagery. Unfortunately, there are many factors that can cause cybersickness and there is no foolproof method for eliminating the problem. In this paper, I discuss a number of the primary factors that contribute to the cause of cybersickness, describe three conflicting cybersickness theories that have been postulated, and discuss some possible methods for reducing cybersickness in VEs.},
   author = {Joseph J. LaViola},
   doi = {10.1145/333329.333344},
   issn = {0736-6906},
   issue = {1},
   journal = {ACM SIGCHI Bulletin},
   month = {1},
   pages = {47-56},
   publisher = {Association for Computing Machinery (ACM)},
   title = {A discussion of cybersickness in virtual environments},
   volume = {32},
   url = {https://dl.acm.org/doi/abs/10.1145/333329.333344},
   year = {2000},
}
@booksection{Golding,
   abstract = {Over 2000 years ago the Greek physician Hippocrates wrote, “sailing on the sea proves that motion disorders the body.” Indeed, the word “nausea” derives from the Greek root word naus, hence “nautical,” meaning a ship. The primary signs and symptoms of motion sickness are nausea and vomiting. Motion sickness can be provoked by a wide variety of transport environments, including land, sea, air, and space. The recent introduction of new visual technologies may expose more of the population to visually induced motion sickness. This chapter describes the signs and symptoms of motion sickness and different types of provocative stimuli. The “how” of motion sickness (i.e., the mechanism) is generally accepted to involve sensory conflict, for which the evidence is reviewed. New observations concern the identification of putative “sensory conflict” neurons and the underlying brain mechanisms. But what reason or purpose does motion sickness serve, if any? This is the “why” of motion sickness, which is analyzed from both evolutionary and nonfunctional maladaptive theoretic perspectives. Individual differences in susceptibility are great in the normal population and predictors are reviewed. Motion sickness susceptibility also varies dramatically between special groups of patients, including those with different types of vestibular disease and in migraineurs. Finally, the efficacy and relative advantages and disadvantages of various behavioral and pharmacologic countermeasures are evaluated.},
   author = {J. F. Golding},
   doi = {10.1016/B978-0-444-63437-5.00027-3},
   issn = {22124152},
   journal = {Handbook of Clinical Neurology},
   keywords = {motion sickness,nausea,transport,vestibular,visual displays,vomiting},
   pages = {371-390},
   pmid = {27638085},
   publisher = {Elsevier B.V.},
   title = {Motion sickness},
   volume = {137},
   year = {2016},
}
@report{Johnson,
   author = {David M Johnson and Zita M Simutis},
   title = {Introduction to and Review of Simulator Sickness Research},
   year = {2005},
}
@book{Reason,
   author = {J.T. Reason and J.J. Brand},
   isbn = {9780125840507},
   publisher = {Academic Press},
   title = {Motion Sickness},
   url = {https://books.google.de/books?id=JMxrAAAAMAAJ},
   year = {1975},
}
@inproceedings{Pohl2021,
   abstract = {Figure 1: Poros enables users to bring portions of distant spaces closer so that they can interact with and across them. Shown here are two proxies, linked to marked spaces (shown in the same color) around two different bookshelves. The user is about to move a book from one space to the other. In addition to direct interactions through them, users can move and arrange proxies, as well as perform operations on them, such as merging and aligning. ABSTRACT A compelling property of virtual reality is that it allows users to interact with objects as they would in the real world. However, such interactions are limited to space within reach. We present Poros, a system that allows users to rearrange space. After marking a portion of space, the distant marked space is mirrored in a nearby proxy. Thereby, users can arrange what is within their reachable space, making it easy to interact with multiple distant spaces as well as nearby objects. Proxies themselves become part of the scene and can be moved, rotated, scaled, or anchored to other objects. Furthermore, they can be used in a set of higher-level interactions such as alignment and action duplication. We show how Poros enables a variety of tasks and applications and also validate its effectiveness through an expert evaluation.},
   author = {Henning Pohl and Klemen Lilija and Jess McIntosh and Kasper Hornbæk},
   city = {New York, NY, USA},
   doi = {10.1145/3411764.3445685},
   isbn = {9781450380966},
   journal = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
   keywords = {3D user interface,portals,virtual reality,worlds in miniature},
   month = {5},
   pages = {1-12},
   publisher = {ACM},
   title = {Poros: Configurable Proxies for Distant Interactions in VR},
   url = {https://dl.acm.org/doi/10.1145/3411764.3445685},
   year = {2021},
}
@inproceedings{Liu2018,
   abstract = {Teleportation is a popular locomotion technique that lets users safely navigate beyond the confines of available positional tracking space without inducing VR sickness. Because available walking space is limited and teleportation is faster than walking, a risk with using teleportation is that users might end up abandoning walking input and only relying on teleportation, which is considered detrimental to presence. We present redirected teleportation; an improved version of teleportation that uses iterative non-obtrusive reorientation and repositioning using a portal to redirect the user back to the center of the tracking space, where available walking space is larger. A user study compares the effectiveness, accuracy, and usability of redirected teleportation with regular teleportation using a navigation task in three different environments. Results show that redirected teleportation allows for a better utilization of available tracking space than regular teleportation, as it requires significantly fewer teleportations, while users walk more and use a larger portion of the available tracking space.},
   author = {James Liu and Hirav Parekh and Majed Al-Zayer and Eelke Folmer},
   city = {New York, NY, USA},
   doi = {10.1145/3242587.3242601},
   isbn = {9781450359481},
   journal = {UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
   keywords = {Locomotion,Positional tracking,Teleportation,Virtual reality},
   month = {10},
   pages = {521-529},
   publisher = {Association for Computing Machinery, Inc},
   title = {Increasing walking in VR using redirected teleportation},
   url = {https://dl.acm.org/doi/10.1145/3242587.3242601},
   year = {2018},
}
@report{Kiyokawa,
   abstract = {This paper proposes a tunnel window, a versatile three dimensional interaction technique, for both navigation and remote object manipulation in a large scale virtual environment. A tunnel window, which is created at an arbitrary position in space using a set of hand gesture commands, provides a secondary view whose viewpoint can be controlled independently from that of the primary view. A tunnel window allows various kinds of seamless teleportation operations including teleportation of the user's viewpoint and that of a virtual object from one position to another by simply passing through the window frame. To extend the interaction scheme, a variety of transformation combinations of the relevant coordinate systems, i.e. those of a user, a window frame, and primary and secondary scenes, have been considered. This paper describes basic concepts and design of a tunnel window interface, its implementation, and variations of the tunnel window scheme.},
   author = {Kiyoshi Kiyokawa and Haruo Takemura},
   title = {A Tunnel Window and Its Variations: Seamless Teleportation Techniques in a Virtual Environment},
}
@webpage{TeleportSystemOverview,
   title = {TeleportSystemOverview | Microsoft Docs},
   url = {https://docs.microsoft.com/en-us/windows/mixed-reality/mrtk-unity/features/teleport-system/teleport-system},
}
@report{Argelaguet,
   abstract = {Virtual environments can be infinitely large, but users only have a limited amount of space in the physical world. One way to navigate within large virtual environments is through teleportation. Teleportation requires two steps: targeting a place and sudden shifting. Conventional teleportation uses a controller to point to a target position and a button press or release to immediately teleport the user to the position. Since the teleportation does not require physical movement, the user can explore the entire virtual environment. However, as this is supernatural and can lead to momentary disorientation, it can break the sense of presence, and thus degrade the overall virtual reality experience. To compensate for the downside of this technique, we explore the effects of a jumping gesture as a teleportation trigger. We conducted a study with two factors: 1) triggering method (Jumping and Standing), and 2) targeting method (Head-direction and Controller). We found that the conventional way of using a controller while standing showed better efficiency, the highest usability and lower cybersickness. Nevertheless, Jump-ing+Controller invoked a high sense of engagement and fun, and therefore provides an interesting new technique, especially for VR games.},
   author = {F Argelaguet and R P Mcmahan and M Sugimoto},
   keywords = {Mixed / augmented reality,Teleportation, Virtual Reality, Locomotion Techniq,Virtual reality},
   title = {On the Use of Jumping Gestures for Immersive Teleportation in VR},
}
@article{Schafer2021,
   abstract = {Virtual Reality (VR) technology offers users the possibility to immerse and freely navigate through virtual worlds. An important component for achieving a high degree of immersion in VR is locomotion. Often discussed in the literature, a natural and effective way of controlling locomotion is still a general problem which needs to be solved. Recently, VR headset manufacturers have been integrating more sensors, allowing hand or eye tracking without any additional required equipment. This enables a wide range of application scenarios with natural freehand interaction techniques where no additional hardware is required. This paper focuses on techniques to control teleportation-based locomotion with hand gestures, where users are able to move around in VR using their hands only. With the help of a comprehensive study involving 21 participants, four different techniques are evaluated. The effectiveness and efficiency as well as user preferences of the presented techniques are determined. Two two-handed and two one-handed techniques are evaluated, revealing that it is possible to move comfortable and effectively through virtual worlds with a single hand only.},
   author = {Alexander Schäfer and Gerd Reis and Didier Stricker},
   doi = {10.3390/electronics10060715},
   keywords = {VR,bare hand,freehand,gestural input,gestures,hands-free,locomotion,movement,navigation,virtual reality},
   title = {Controlling Teleportation-Based Locomotion in Virtual Reality
with Hand Gestures: A Comparative Evaluation of
Two-Handed and One-Handed Techniques},
   url = {https://doi.org/10.3390/electronics10060715},
   year = {2021},
}
@webpage{Interhaptics,
   title = {Hand Tracking for Virtual Reality (VR) \& Mixed Reality (MR) | Interhaptics - Haptics and Interactions for Virtual Reality (VR) and Mixed Reality (MR)},
   url = {https://www.interhaptics.com/products/hand-tracking-for-vr-and-mr},
}
@article{Ardito2014,
   abstract = {Recent advances in computing devices push researchers to envision new interaction modalities that go beyond traditional mouse and keyboard input. Typical examples are large displays for which researchers hope to create more "natural" means of interaction by using human gestures and body movements as input. In this article, we reflect about this goal of designing gestures that people can easily understand and use and how designers of gestural interaction can capitalize on the experience of 30 years of research on visual languages to achieve it. Concretely, we argue that gestures can be regarded as "visual expressions to convey meaning" and thus are a visual language. Based on what we have learned from visual language research in the past, we then explain why the design of a generic gesture set or language that spans many applications and devices is likely to fail. We also discuss why we recommend using gestural manipulations that enable users to directly manipulate on-screen objects instead of issuing commands with symbolic gestures whose meaning varies among different users, contexts, and cultures.},
   author = {Carmelo Ardito and Maria Francesca Costabile and Hans Christian Jetter},
   doi = {10.1016/j.jvlc.2014.07.002},
   issn = {1045926X},
   issue = {5},
   journal = {Journal of Visual Languages and Computing},
   keywords = {Gestural languages,Manipulations,Symbolic gestures,Visual expressions},
   month = {10},
   pages = {572-576},
   publisher = {Academic Press},
   title = {Gestures that people can understand and use},
   volume = {25},
   year = {2014},
}
@inproceedings{Stern2006,
   abstract = {A global approach to hand gesture vocabulary design is proposed which includes human as well as technical design factors. The method of selecting gestures for preconceived command vocabularies has not been addressed in a systematic manner. Present methods are ad hoc. In an analytical approach technological factors of gesture recognition accuracy are easily obtained and well studied. Conversely, it is difficult to obtain measures of human centered desires (intuitiveness, comfort), These factors, being subjective, are costly and time consuming to obtain, and hence we have developed automated methods for acquisition of these data through specially designed applications. Results of the intuitiveness experiments showed when commands are presented as stimuli the gestural responses vary widely over a population of subjects. This result refutes the hypothesis that there exist universal common gestures to express user intentions or commands. © 2006 IEEE.},
   author = {Helman I. Stern and Juan P. Wachs and Yael Edan},
   doi = {10.1109/ICSMC.2006.384767},
   isbn = {1424401003},
   issn = {1062922X},
   journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
   keywords = {Hand gesture,Human factors,Intuitive interfaces,Man-machine interaction,Optimal vocabulary},
   month = {10},
   pages = {4052-4056},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Human factors for design of hand gesture human - Machine interaction},
   volume = {5},
   url = {http://ieeexplore.ieee.org/document/4274532/},
   year = {2006},
}
@article{Lu2020,
   abstract = {Hand gestures provide a natural and easy-to-use way to input commands. However, few works have studied the design space of bimanual hand gestures or attempted to infer gestures that involve devices on both hands. We explore the design space of hand-to-hand gestures, a group of gestures that are performed by touching one hand with the other hand. Hand-to-hand gestures are easy to perform and provide haptic feedback on both hands. Moreover, hand-to-hand gestures generate simultaneous vibration on two hands that can be sensed by dual off-the-shelf wrist-worn devices. In this work, we derive a hand-to-hand gesture vocabulary with subjective ratings from users and select gesture sets for real-life scenarios. We also take advantage of devices on both wrists to demonstrate their gesture-sensing capability. Our results show that the recognition accuracy for fourteen gestures is 94.6\% when the user is stationary, and the accuracy for five gestures is 98.4\% or 96.3\% when the user is walking or running, respectively. This is significantly more accurate than a single device worn on either wrist. Our further evaluation also validates that users can easily remember hand-to-hand gestures and use our technique to invoke commands in real-life contexts.},
   author = {Yiqin Lu and Bingjian Huang and Chun Yu and Guahong Liu and Yuanchun Shi},
   doi = {10.1145/3380984},
   journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol},
   keywords = {hand gesture,locomotion,motion correlation,wearable device},
   pages = {27},
   title = {Designing and Evaluating Hand-to-Hand Gestures with Dual Commodity Wrist-Worn Devices},
   volume = {4},
   url = {https://doi.org/10.1145/3380984},
   year = {2020},
}
@inproceedings{Vatavu2012,
   abstract = {As researchers and industry alike are proposing TV interfaces that use gestures in their designs, understanding users' preferences for gesture commands becomes an important problem. However, no rules or guidelines currently exist to assist designers and practitioners of such interfaces. The paper presents the results of the first study investigating users' preferences for free-hand gestures when controlling the TV set. By conducting an agreement analysis on user-elicited gestures, a set of gesture commands is proposed for basic TV control tasks. Also, guidelines and recommendations issued from observed user behavior are provided to assist practitioners interested in prototyping free-hand gestural designs for the interactive TV. © 2012 ACM.},
   author = {Radu Daniel Vatavu},
   city = {New York, New York, USA},
   doi = {10.1145/2325616.2325626},
   isbn = {9781450311076},
   journal = {EuroiTV'12 - Proceedings of the 10th European Conference on Interactive TV and Video},
   keywords = {Kinect,TV,experiment,free-hand,gesture recognition,gestures,guessability,interactive TV,living room,study,user-defined},
   pages = {45-48},
   publisher = {ACM Press},
   title = {User-defined gestures for free-hand TV control},
   url = {http://dl.acm.org/citation.cfm?doid=2325616.2325626},
   year = {2012},
}
@booksection{Lin2017,
   abstract = {Virtual reality requires high levels of interaction with the user, a type of human computer interaction. Interactions that match the way humans usually interact with their surroundings should improve training effectiveness. A 3D hand gesture based interface allows users to control the position and orientation of 3D objects by simply moving their hands, thereby, creating a more naturalistic interaction process. The design of hand gestures should be evaluated to determine design features that are the most effective and comfortable for the user. The purpose of this study was to evaluate parameters for the design of 3D hand gestures for object manipulation in virtual reality to optimize productivity and usability. Twenty participants completed object manipulation tasks while wearing an Oculus Rift headset with a mounted Leap Motion depth sensor camera to capture hand gestures. Independent variables were distance from hand to object, hand posture threshold for grab and release, and grab locations on object. The dependent variables were time for task completion and subjective measures of control, fatigue, motion sickness, and preference. The preferred gesture design parameter was related to better control and reduced time to complete the tasks. In conclusion, this study identified important gesture design features that can be optimized to improve usability and throughput for an object manipulation task in Virtual Reality.},
   author = {Wanhong Lin and Lear Du and Carisa Harris-Adamson and Alan Barr and David Rempel},
   doi = {10.1007/978-3-319-58071-5_44},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {3D hand gestures,Hand postures,Human-computer interaction},
   pages = {584-592},
   publisher = {Springer Verlag},
   title = {Design of hand gestures for manipulating objects in virtual reality},
   volume = {10271},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-58071-5_44},
   year = {2017},
}
@article{Pereira2015,
   abstract = {Objective: The purpose of this study was to develop a lexicon for 3-D hand gestures for common humancomputer interaction (HCI) tasks by considering usability and effort ratings. Background: Recent technologies create an opportunity for developing a free-form 3-D hand gesture lexicon for HCI. Method: Subjects (N = 30) with prior experience using 2-D gestures on touch screens performed 3-D gestures of their choice for 34 common HCI tasks and rated their gestures on preference, match, ease, and effort. Videos of the 1,300 generated gestures were analyzed for gesture popularity, order, and response times. Gesture hand postures were rated by the authors on biomechanical risk and fatigue. Results: A final task gesture set is proposed based primarily on subjective ratings and hand posture risk. The different dimensions used for evaluating task gestures were not highly correlated and, therefore, measured different properties of the taskgesture match. Application: A method is proposed for generating a user-developed 3-D gesture lexicon for common HCIs that involves subjective ratings and a posture risk rating for minimizing arm and hand fatigue.},
   author = {Anna Pereira and Juan P. Wachs and Kunwoo Park and David Rempel},
   doi = {10.1177/0018720814559307},
   issn = {15478181},
   issue = {4},
   journal = {Human Factors},
   keywords = {HCI,fatigue,gesture,human-computer interaction,usability},
   month = {6},
   pages = {607-621},
   pmid = {25977321},
   publisher = {SAGE Publications Inc.},
   title = {A user-developed 3-D hand gesture set for human-computer interaction},
   volume = {57},
   url = {http://journals.sagepub.com/doi/10.1177/0018720814559307},
   year = {2015},
}
@article{Rempel2014,
   abstract = {The design and selection of 3D modeled hand gestures for human-computer interaction should follow principles of natural language combined with the need to optimize gesture contrast and recognition. The selection should also consider the discomfort and fatigue associated with distinct hand postures and motions, especially for common commands. Sign language interpreters have extensive and unique experience forming hand gestures and many suffer from hand pain while gesturing. Professional sign language interpreters (N=24) rated discomfort for hand gestures associated with 47 characters and words and 33 hand postures. Clear associations of discomfort with hand postures were identified. In a nominal logistic regression model, high discomfort was associated with gestures requiring a flexed wrist, discordant adjacent fingers, or extended fingers. These and other findings should be considered in the design of hand gestures to optimize the relationship between human cognitive and physical processes and computer gesture recognition systems for human-computer input. © 2014 Elsevier Ltd.},
   author = {David Rempel and Matt J. Camilleri and David L. Lee},
   doi = {10.1016/j.ijhcs.2014.05.003},
   issn = {10959300},
   issue = {10-11},
   journal = {International Journal of Human Computer Studies},
   keywords = {Computer input,Computer interface,Gesture-based interaction,Hand postures,Multi-touch},
   month = {10},
   pages = {728-735},
   publisher = {Academic Press},
   title = {The design of hand gestures for human-computer interaction: Lessons from sign language interpreters},
   volume = {72},
   year = {2014},
}
@webpage{handstworelease,
   title = {Presence Platform’s Hand Tracking API Gets an Upgrade},
   url = {https://developer.oculus.com/blog/presence-platforms-hand-tracking-api-gets-an-upgrade},
}
@article{Bustamante2008,
   abstract = {Mental workload is one of the most important constructs of interests for Human Factors researchers. Adequately assessing the amount of mental workload that people experience while performing tasks ...},
   author = {Ernesto A. Bustamante and Randall D. Spain},
   doi = {10.1177/154193120805201946},
   isbn = {9781605606859},
   issn = {10711813},
   journal = {http://dx.doi.org/10.1177/154193120805201946},
   month = {9},
   pages = {1522-1526},
   publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
   title = {Measurement Invariance of the Nasa TLX:},
   volume = {3},
   url = {https://journals.sagepub.com/doi/abs/10.1177/154193120805201946?casa_token=XSyBSmtaidcAAAAA%3Atz8DfFu452M04GXaEoUQMe738WjZntI7LaU6ZVodkJDnOfTL2sRdkAddpQ975xrOKbZe9fv18Vq8zQ},
   year = {2008},
}
@article{Hart1988,
   abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload. © 1988 Elsevier Science & Technology.},
   author = {Sandra G. Hart and Lowell E. Staveland},
   doi = {10.1016/S0166-4115(08)62386-9},
   issn = {0166-4115},
   issue = {C},
   journal = {Advances in Psychology},
   month = {1},
   pages = {139-183},
   publisher = {North-Holland},
   title = {Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research},
   volume = {52},
   year = {1988},
}
